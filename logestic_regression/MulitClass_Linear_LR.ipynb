{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement this previous code with class, in this case, the code would be clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_dataset():\n",
    "    dataset= datasets.load_iris() # Dataset normalized with Standard Deviation Scaler\n",
    "    features= dataset['feature_names']\n",
    "    final_dataset= pd.DataFrame(dataset['data'], columns=[features])\n",
    "    final_dataset= final_dataset.assign(target= dataset['target'])\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target\n",
       "0               5.1              3.5               1.4              0.2      0\n",
       "1               4.9              3.0               1.4              0.2      0\n",
       "2               4.7              3.2               1.3              0.2      0\n",
       "3               4.6              3.1               1.5              0.2      0\n",
       "4               5.0              3.6               1.4              0.2      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset= input_dataset()\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_dataset= final_dataset.iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(final_dataset[['target']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   (sepal length (cm),)  150 non-null    float64\n",
      " 1   (sepal width (cm),)   150 non-null    float64\n",
      " 2   (petal length (cm),)  150 non-null    float64\n",
      " 3   (petal width (cm),)   150 non-null    float64\n",
      " 4   (target,)             150 non-null    int32  \n",
      "dtypes: float64(4), int32(1)\n",
      "memory usage: 5.3 KB\n"
     ]
    }
   ],
   "source": [
    "final_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(final_dataset.iloc[:, :-1]\n",
    "                                                   ,final_dataset.iloc[:, -1], test_size= 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "val= np.arange(-10, 10 , .01)\n",
    "z = 1 / ( 1+ np.exp(- val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val, z)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(X):\n",
    "    '''\n",
    "    In this function we use Standard Scaler for scaling test set and train set.\n",
    "    --------------------------\n",
    "    Parameter:\n",
    "    X: Test set or train set\n",
    "    --------------------------\n",
    "    return:\n",
    "    scaled data\n",
    "    '''\n",
    "    X_scaled= StandardScaler().fit_transform(X)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Linear Logistic Regression hypothesis:</strong>\n",
    "<h2>\n",
    "    <p style=\"color:red\">\n",
    "    h( <font face= \"Symbol\">q</font><sup>T</sup>x)= g (<font face= \"Symbol\">q</font><sup>T</sup>x)= <sup>1</sup>&frasl;<sub> 1 + e<sup> -<font face= \"Symbol\">q</font><sup>T</sup>x</sup></sub>\n",
    "    </p>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate= .04, max_iter= 15000):\n",
    "        '''\n",
    "        This class is LogisticRegression with full batch gradient descent algorithm and predict just Binary Class.\n",
    "        for multi class dataset we jsut use OneVSRest classifier class.\n",
    "        -----------------------\n",
    "        Parameters:\n",
    "        learning_rate(default .001): steps.please use optimized value to learning rate for faster learning and Prevent divergence.\n",
    "        max_iter(default 1500): number of times that we want run gradient descent.\n",
    "        <<Developer Name: Mohammad Karami Sheykhlan based on Stanford University course( Andrew Ng )>>\n",
    "        '''\n",
    "        self._learning_rate= learning_rate\n",
    "        self._max_iter= max_iter\n",
    "#         self._theta_value= []\n",
    "        self._cost_value= []\n",
    "        self._theta= []\n",
    "        self.predict_proba= []\n",
    "    #---------------------------------------------------------------------------------   \n",
    "    def _sigmoid(self, X):\n",
    "        '''\n",
    "        This function is h_theta_x (prediction) that we use sigmoid(regression) formula.\n",
    "        The formula is 1 / (1 + e ** -(X_train or X_test @ theta ))\n",
    "        After finding theta's, we can use this function for predict for X_test too.\n",
    "        ----------\n",
    "        Parameter:\n",
    "        X: X_train or X_test @ theta\n",
    "        -----------\n",
    "        return:\n",
    "        this function return value of sigmoid function (h_x)... \n",
    "        '''\n",
    "        pred= 1 / (1 + np.exp(-X))\n",
    "        \n",
    "        return pred\n",
    "    #---------------------------------------------------------------------------------\n",
    "    def _cost_function(self,X_train, y_train, theta):\n",
    "        '''\n",
    "        In this fucntion we calcualate cost function. We must check converge with drawing cost function.\n",
    "        If it was upwards, it means that divergence has occurred.\n",
    "        If it is down, it must remain constant for a while.\n",
    "        va age be samte paein bud, nabayad kheyli tiz bashad  dar in surat bayad meghdare alpha ro down konim.(persian :))\n",
    "        ---------------------------\n",
    "        parameters:\n",
    "        X_train, y_train, theta\n",
    "        ---------------------------\n",
    "        return:\n",
    "        value of Loss function (J_theta)... 1 * 1 matrix\n",
    "        '''\n",
    "        m= X_train.shape[0]\n",
    "        cost= (-1/m) * (y_train.T @ np.log(self._sigmoid(X_train @ theta)) + (1 - y_train.T) @ np.log(1 - self._sigmoid(X_train @ theta)))\n",
    "        \n",
    "        return cost[0][0]\n",
    "    #---------------------------------------------------------------------------------\n",
    "    def _cost_func_derivative(self, X_train, y_train, theta):\n",
    "        '''\n",
    "        Derivative of Cost Function for Logistic Regression:\n",
    "        dJ_theta(x)⁄dtheta = 1⁄m * x.T (h_theta(x) - y)\n",
    "        '''\n",
    "        m= X_train.shape[0]\n",
    "        deff= X_train.T @ (self._sigmoid(X_train @ theta) - y_train)\n",
    "\n",
    "        return deff\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        In this function we want to find optimized theta values with Gradient descent.\n",
    "        Before everything, we add intercept(1) to X_train matrix then we set random values to theta.\n",
    "        ---------------------\n",
    "        Parameters:\n",
    "        X_train: maxtirx of features that we want to learn to computer\n",
    "        y_train: value of target \n",
    "        ---------------------   \n",
    "        '''\n",
    "        X_train= np.insert(X_train, 0, 1, axis= 1) # add new col in X_train[:, 0] value is 1\n",
    "        num_of_features= X_train.shape[1]\n",
    "        m=  X_train.shape[0]\n",
    "        theta= np.random.rand(num_of_features, 1)\n",
    "        cost_value= []\n",
    "        theta_value= []\n",
    "\n",
    "        for i in range(self._max_iter):\n",
    "            cost= self._cost_function(X_train, y_train.reshape(len(X_train), 1), theta)\n",
    "            self._cost_value.append(cost)\n",
    "            theta= theta - (self._learning_rate * (1 / m)) * self._cost_func_derivative(X_train, y_train.reshape(len(X_train), 1), theta)\n",
    "            theta_value.append(theta)\n",
    "            \n",
    "        self._theta= theta_value[-1]    \n",
    "    #--------------------------------------------------------------------------------------\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        This function return prediction values for X_test. \n",
    "        --------------------\n",
    "        Parameters:\n",
    "        X_test: The dataset on which we want to run the regression logistics algorithm.\n",
    "        theta: FINAL value for theta.\n",
    "        --------------------\n",
    "        return:\n",
    "        Return prediction Matrix about X_test it is 0 / 1. and returned matrix size is (m,) that m is number of X_test row.\n",
    "        '''\n",
    "        X_test= np.insert(X_test, 0, 1, axis= 1)\n",
    "        self.predict_proba= self._sigmoid(X_test @ self._theta)\n",
    "        norm_pred= np.where(self.predict_proba < .5, 0, 1) # 0 if pred < .5 else 1\n",
    "\n",
    "        return norm_pred.ravel()\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    def accuracy(self, y_pred, y_test):\n",
    "        '''\n",
    "        The model accuracy formula is: number_of_corect_prediction / number_of_all_test_set_sample\n",
    "        '''\n",
    "        correct_predict_count= np.equal(y_pred, y_test).sum()\n",
    "        return correct_predict_count / len(y_test)\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    def draw_cost_function(self):\n",
    "        cost= np.array(self._cost_value)\n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Num. of iter')\n",
    "        plt.ylabel('cost (J_theta)')\n",
    "        plt.title('Cost Function')\n",
    "        plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std= scaler(X_train)\n",
    "X_test_std= scaler(X_test)\n",
    "lr= LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneVSRestClassifier:\n",
    "    def __init__(self, model):\n",
    "        '''\n",
    "        In this class we implement One VS. Rest(All) classifier.       \n",
    "        -----------------\n",
    "        Parameter:\n",
    "        model: The model name that we want to use(for example: Logistic regression, SVM, MLP and etc.)\n",
    "        \n",
    "        << Developer Name: Mohammad Karami Sheykhlan based on Stanford University course( Andrew Ng ) >>\n",
    "        '''\n",
    "        self.model= model\n",
    "        self.thet= []\n",
    "        self.classes= 0\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        In this function, for each class we calculate optimazed theta\n",
    "        '''\n",
    "        self.classes= np.unique(y_train)\n",
    "#         print(self.classes)\n",
    "        \n",
    "        for index, i in enumerate(self.classes):\n",
    "            new_y_test= (y_train == i).astype(int) # for 'i'th element value we put 1 and anothers 0\n",
    "            self.model.fit(X_train, new_y_test) # then we find optimized value for theta for every class\n",
    "            self.thet.append(self.model._theta.ravel())\n",
    "#             self.model.draw_cost_function()\n",
    "            self.model._cost_value= []\n",
    "        self.thet= np.array(self.thet).T\n",
    "        print('Theta values for every class is:\\n', self.thet)\n",
    "    def predict(self, X_test):\n",
    "        X_test= np.insert(X_test, 0, 1, axis= 1)\n",
    "        self.predict_proba= self.model._sigmoid(X_test @ self.thet)\n",
    "        y_pred= np.argmax(self.predict_proba, axis= 1)\n",
    "        \n",
    "        return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr= OneVSRestClassifier(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta values for every class is:\n",
      " [[-3.19673156 -0.88343559 -6.58358988]\n",
      " [-1.61701458 -0.13397421 -0.62883728]\n",
      " [ 2.47232568 -1.30768169 -0.89640889]\n",
      " [-2.98181446  2.01232     4.75554336]\n",
      " [-2.83690366 -1.72909145  5.82974284]]\n"
     ]
    }
   ],
   "source": [
    "ovr.fit(X_train_std, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= ovr.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.accuracy(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Linear Logistic Regression Loss function:</strong>\n",
    "<h2>\n",
    "    <p style=\"color:red\">\n",
    "        J(<font face=\"Symbol\">q</font>)=<sup>1</sup>&frasl;<sub>m</sub>&sum;<sup>m</sup><sub>i=1</sub>[ -y<sup>i</sup>log<sup>h<sub><font face=\"Symbol\">q</font></sub>(x<sup>i</sup>)</sup> - ( 1 - y<sup>i</sup>)log<sup>(1 - h<font face=\"Symbol\">q</font>(x<sup>i</sup>))</sup>]\n",
    "    </p>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>\n",
    "    Derivative of Cost Function for Logistic Regression:\n",
    "</strong>\n",
    "<h2>\n",
    "    <p style=\"color:red\">\n",
    "        <sup>dJ<sub><font face=\"Symbol\">q</font></sub>(x)</sup>&frasl;<sub>d<font face=\"Symbol\">q</font></sub> = <sup>1</sup>&frasl;<sub>m</sub> x<sup>T</sup> (h<sub><font face=\"Symbol\">q</font></sub>(x) - y)\n",
    "    </p>\n",
    "</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
